{"posts":[{"title":"Model Context Protocol (MCP)","text":"A Protocol for Simplifying LLM Application Development. Why MCP?Before MCP, most LLM apps rebuilt the same glue: Resource collection ‚Äî where to pull context from (files, databases, calendars, screens, etc.) Tool wiring ‚Äî how to call external services (search, code exec, web fetch, RAG, ‚Ä¶) Prompt shape ‚Äî how to consistently describe resources and tools to the model That‚Äôs brittle and expensive. MCP standardizes the surface so apps can: discover what data and tools exist, 2) describe them to the model, and 3) route tool calls‚Äîwithout app-specific adapters. Core concepts Resources ‚Äî things you can read to build context (files, DB tables, dashboards, device state‚Ä¶).Each resource has an ID, human description, optional schema/metadata, and a way to fetch it. Tools ‚Äî actions the model can call (search, translate, execute, query DB, open URL‚Ä¶).A tool has a name, description, arguments schema, and effect flags (e.g., ‚Äúreads external data‚Äù, ‚Äúmutates state‚Äù). Prompts ‚Äî reusable prompt snippets MCP servers can publish (e.g., ‚ÄúSQL safety checklist‚Äù) for high-quality, repeatable guidance. Mental model123LLM app ‚îÄ‚ñ∫ MCP client ‚îÄ‚ñ∫ (one or more) MCP servers ‚îú‚îÄ Resources (discover/fetch) ‚îî‚îÄ Tools (list/execute) Your app speaks only MCP. Servers implement whatever backends they want. Resource examplesMCP doesn‚Äôt force one URI scheme; servers should expose stable, URI-like IDs: 123file:///home/reda/notes/mcp_todos.mdpostgres://prod/orders?table=orders_2025screen://localhost/display1 Minimal resource descriptor (illustrative): 1234567{ &quot;id&quot;: &quot;file:///home/reda/notes/mcp_todos.md&quot;, &quot;title&quot;: &quot;MCP TODOs&quot;, &quot;description&quot;: &quot;Personal notes about tasks and follow-ups.&quot;, &quot;mime&quot;: &quot;text/markdown&quot;, &quot;size&quot;: 4821} Fetching returns a payload (text or bytes) plus metadata: 123456{ &quot;id&quot;: &quot;file:///home/reda/notes/mcp_todos.md&quot;, &quot;content&quot;: &quot;# TODO\\n- write MCP post\\n- push to blog\\n&quot;, &quot;etag&quot;: &quot;b2f9c3&quot;, &quot;updated_at&quot;: &quot;2025-09-14T10:21:00Z&quot;} Tool examplesA tool is described once; the model can then call it through the MCP client. 123456789101112{ &quot;name&quot;: &quot;web_search&quot;, &quot;description&quot;: &quot;Search the web and return top snippets.&quot;, &quot;arguments&quot;: { &quot;query&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Search query&quot; }, &quot;top_k&quot;: { &quot;type&quot;: &quot;integer&quot;, &quot;default&quot;: 5, &quot;minimum&quot;: 1, &quot;maximum&quot;: 20 } }, &quot;effects&quot;: { &quot;reads_external_data&quot;: true, &quot;mutates_state&quot;: false }} Another: safe SQL query tool ‚Äî 12345678910{ &quot;name&quot;: &quot;query_orders&quot;, &quot;description&quot;: &quot;Run a parameterized query on the Orders view.&quot;, &quot;arguments&quot;: { &quot;from&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;format&quot;: &quot;date&quot; }, &quot;to&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;format&quot;: &quot;date&quot; }, &quot;status&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;open&quot;, &quot;shipped&quot;, &quot;canceled&quot;] } }, &quot;effects&quot;: { &quot;reads_external_data&quot;: true }} Typical client flow Discover ‚Äî list resources/tools from each MCP server. Select ‚Äî for a user query, choose relevant resources (by rules or retrieval) and a small tool set. Describe ‚Äî build a compact context card for the model: names + short descriptions of selected resources/tools any safety constraints / rate limits Call the model ‚Äî provide the user message + context card. Route tool calls ‚Äî execute via the MCP client; capture results (including errors) and stream them back to the model. Answer ‚Äî return final text; optionally attach citations (resource IDs) and traces. Prompt snippetsServers can publish helpful prompts the client can insert: 12345{ &quot;name&quot;: &quot;sql_safety_prompt&quot;, &quot;description&quot;: &quot;Safety checklist for SQL generation&quot;, &quot;arguments&quot;: []} Example usage in a system message: When using query_orders, never select *. Only named columns and limit to 200 rows.Use prepared parameters {from}, {to}, {status}. Minimal ‚ÄúHello MCP‚Äù (pseudo-code)12345678910111213141516171819202122user = &quot;Show shipped orders this week and summarize revenue.&quot;servers = mcp.discover() # list MCP serversresources = mcp.search_resources(&quot;orders&quot;) # pick relevant resourcestools = [&quot;query_orders&quot;] # expose a small, safe toolsetcontext_card = { &quot;resources&quot;: [r.brief() for r in resources], &quot;tools&quot;: [mcp.describe_tool(t) for t in tools], &quot;prompts&quot;: [mcp.get_prompt(&quot;sql_safety_prompt&quot;)]}reply = llm.chat( system = build_system(context_card), user = user, tools = tools)for call in reply.tool_calls: res = mcp.run(call.name, call.args) reply = llm.continue_with_tool_result(call.id, res)print(reply.final_text) Design tips Keep the context small. Prefer brief descriptors + on-demand fetching. Constrain tools. Expose only what‚Äôs needed per turn; validate arguments against the schema. Trace everything. Log resource IDs, tool calls, and inputs/outputs for reproducibility. Fail loudly. Propagate errors to the model with clear messages; retry selectively. Cache. De-duplicate resource fetches by etag or hash. ConclusionsMCP reduces bespoke glue in LLM apps. You standardize how you discover data/tools, describe them to the model, and route tool calls. The result is simpler apps, safer execution paths, and easier portability across backends. Next steps (what I‚Äôll build) A small MCP server exposing: orders Postgres view as resources query_orders and web_search tools A client that prints a full trace (resource IDs, tool args, response times) Blog follow-ups: security boundaries, rate-limits, and evals for tool-use References Official docs: https://modelcontextprotocol.io/docs/getting-started/intro","link":"/2025/09/14/Model-Context-Protocol-MCP/"},{"title":"Demo Project ‚Äî Real-Time Image Segmentation","text":"A short summary of the project. What it does, why it‚Äôs interesting, and the result. Highlights Lightweight PyTorch model Real-time inference on CPU Simple REST API wrapper Tech StackPyTorch ¬∑ FastAPI ¬∑ ONNX Runtime","link":"/2025/09/14/demo-project/"},{"title":"Essay: Why Imperfect Action Beats Perfect Inaction","text":"Your essay text‚Ä¶","link":"/2025/09/12/first-essay/"},{"title":"Reading Notes: A Paper I Like","text":"Your reading notes‚Ä¶","link":"/2025/09/12/first-reading/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2025/09/13/hello-world/"},{"title":"Line Follower &#x2F; Maze Solver ‚Äî Polymaze 2023","text":"Autonomous line-following robot that detects intersections, maps the maze, then re-runs the shortest path. Built for Polymaze 2023. OverviewThe robot follows a black line using PID on an IR sensor array, detects nodes (R/L/T intersections, turns, and dead ends), saves the path to EEPROM, prunes wrong turns/backtracks, then computes the shortest path and performs an optimized second run. Event: Polymaze 2023 Goal: Fastest complete maze Role: Design, control &amp; software Hardware Microcontroller: Arduino Uno Sensors: 5-element IR array Drive: 2√ó DC motors + L298N driver Power: 3S LiPo Chassis: Laser-cut Control &amp; SensingLine tracking (PID): proportional‚Äìintegral‚Äìderivative control on the line error from the IR array.Node detection: state machine for junction types (R/L/T/cross), debounced by distance/time.Mapping: record moves as symbols (e.g., L, R, S, B for backtrack) in EEPROM.Path optimization: compress the sequence by removing backtrack pairs and simplifying to the minimal route for the second run.","link":"/2023/07/20/polymaze-2023/"},{"title":"Welcome to Reda&#39;s Log Book","text":"This site is built with Hexo and the Icarus theme to share my research, projects, and reading notes.","link":"/2025/09/12/welcome/"},{"title":"Eurobot 2025 ‚Äî Autonomous Mobile Robot (ROS)","text":"A fully ROS-based autonomous robot for Eurobot 2025.We built a robust navigation stack with sensor fusion (wheel odometry + IMU), global/local planning using move_base, and a custom perception package that aligns the robot precisely with the bo√Ætes de conservation (storage boxes) that carry ArUco tags for fast, reliable docking. System overview OS / Framework: Ubuntu + ROS (Noetic/ROS1) Compute: Jetson Nano Sensors: Encoders (odometry), steppers, IMU, RGB camera for ArUco Actuation: 2-wheel differential drive + gripper/mechanism Localization (sensor fusion)We fuse wheel odometry and IMU with robot_localization (EKF).This yields a stable /odometry/filtered and TF from odom ‚Üí base_link. Navigation (move_base) Global planner: navfn Local planner: TEB (teb_local_planner) for smooth, time-elastic trajectories. Perception (ArUco alignment)A lightweight node detects the ArUco tag on the bo√Æte, estimates T(base_link ‚Üí tag) with OpenCV, publishes a target pose in map, then sends a move_base goal.For final centimeters, a short visual-servo velocity loop refines the pose. Big Thanks üíôSincere thanks to: Talebot team ‚Äî for the long nights, tests, and constant support. Ecole Nationale Polytechnique and all sponsors how aide this project. Mentors, volunteers, and friends who gave feedback, tools, and encouragement. Everyone following the project ‚Äî your interest keeps us building!","link":"/2025/05/20/eurobot-2025-autonomy/"},{"title":"Stereo ArUco Robot Posestimation ‚Äî Eurobot 2025 Field Computer","text":"A field-side tracking system that estimates each robot‚Äôs (x, y, yaw) in real time using a stereo pair of USB cameras and ArUco markers. The map is anchored with four fixed markers; each robot carries one marker on the roof. This project is designed to act as the central compute (referee/telemetry helper) for our Eurobot 2025 team. Goals Robust global pose for one or more robots: (x, y, Œ∏) Simple, low-cost hardware (two USB cameras, off-the-shelf lenses) Quick setup at the venue and fast re-calibration when needed Hardware Stereo camera rig: 2√ó USB cameras on a rigid bar with a fixed baseline Mounting: Overlooking the game area (a small downward tilt is fine) Markers: 4√ó field markers at known field coordinates (map anchors) 1√ó marker per robot (top-mounted, high contrast) Pipeline Stereo calibration Calibrate each camera (intrinsics &amp; distortion) and the extrinsics between them. Use a chessboard (or ChArUco) and store K1, D1, K2, D2, R, T. Reference I used: this YouTube serie (clear and practical):https://www.youtube.com/playlist?list=PL2zRqk16wsdoCCLpou-dGo7QQNks1Ppzo ArUco detection (both images) Detect field anchors and robot markers with cv2.aruco.detectMarkers. Estimate per-marker pose in each camera with cv2.aruco.estimatePoseSingleMarkers. Map frame With the four known field marker IDs and their world poses, compute or refresh the camera-to-world transform (PnP / solvePnP; optionally average from multiple anchors). If desired, fuse left/right views via stereo triangulation for extra robustness. Robot pose Transform the robot marker pose into the world (field) frame ‚Üí (x, y, yaw). Yaw is extracted from the rotation vector (rvec) ‚Üí rotation matrix ‚Üí Z-yaw. Results Stable tracking with centimeter-level repeatability on a 2√ó3 m field (after calibration) Yaw is stable within a few degrees; larger markers and careful placement improve it Supports multiple robots simultaneously (distinct marker IDs) Images from testing Next steps Use a Jetson Nano instead of personal laptops as the field computer Leverage CUDA to parallelize heavy steps and reduce latency Optional: share robot/world poses over Wi-Fi/ROS so our robots know their ownposition and the opponents‚Äô positions in real time","link":"/2025/01/15/aruco-stereo-tracking/"},{"title":"Ocean Cleaning Robot ‚Äî Algerian Robot Cup 2024","text":"Tele-operated robot built for the Algerian Robot Cup (ARC) 2024 to collect floating waste from water surfaces and deliver it to a collection zone. Reliable, easy to service between runs, and tuned for smooth operator control. Competition context Event: Algerian Robot Cup (ARC) 2024 Challenge: Remove floating debris from a pool and deposit it in a target area within the time limit Location: Algiers, Algeria Role: Mechanical &amp; electronics design, integration, and driver Mechanical design Amphibious chassis that can traverse gravel and sand near the water‚Äôs edge and operate effectively on the water surface Wide funnel/scoop guiding debris into a removable basket for fast turn-around Serviceability: quick-release modules and accessible wiring to minimize pit time Electronics &amp; control Land drive: 4√ó DC motors for traction on gravel and sand Water propulsion: 2√ó brushless motors + ESCs for maneuvering in water Actuation: high-torque servo for scoop angle; stepper motor to eject (throw) stacked dishes into the trash basket Tele-op link: nRF24L01 2.4 GHz; tuned with input expo/rate limiting for smooth control Results Consistent pickups across mixed debris sizes Only team to pick multiple dishes, stack them on board, and then throw them all into the trash at once Drawback: the robot was heavy, so top speed was lower Lessons learned Reduce mass (lighter deck, optimized battery size, selective material swaps) to increase speed and maneuverability Improve sealing and splash protection around moving shafts and connectors Add voltage/current telemetry to prevent late-match brownouts Media Photo: Algerian Robot Cup (ARC) 2024.","link":"/2024/07/20/ocean-cleaning-robot/"}],"tags":[{"name":"MCP","slug":"MCP","link":"/tags/MCP/"},{"name":"Model Context Protocol","slug":"Model-Context-Protocol","link":"/tags/Model-Context-Protocol/"},{"name":"LLM","slug":"LLM","link":"/tags/LLM/"},{"name":"tools","slug":"tools","link":"/tags/tools/"},{"name":"context","slug":"context","link":"/tags/context/"},{"name":"computer vision","slug":"computer-vision","link":"/tags/computer-vision/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"segmentation","slug":"segmentation","link":"/tags/segmentation/"},{"name":"thoughts","slug":"thoughts","link":"/tags/thoughts/"},{"name":"notes","slug":"notes","link":"/tags/notes/"},{"name":"robotics","slug":"robotics","link":"/tags/robotics/"},{"name":"line follower","slug":"line-follower","link":"/tags/line-follower/"},{"name":"maze","slug":"maze","link":"/tags/maze/"},{"name":"embedded","slug":"embedded","link":"/tags/embedded/"},{"name":"polymaze","slug":"polymaze","link":"/tags/polymaze/"},{"name":"welcome","slug":"welcome","link":"/tags/welcome/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"Eurobot 2025","slug":"Eurobot-2025","link":"/tags/Eurobot-2025/"},{"name":"ROS","slug":"ROS","link":"/tags/ROS/"},{"name":"robot_localization","slug":"robot-localization","link":"/tags/robot-localization/"},{"name":"EKF","slug":"EKF","link":"/tags/EKF/"},{"name":"IMU","slug":"IMU","link":"/tags/IMU/"},{"name":"odometry","slug":"odometry","link":"/tags/odometry/"},{"name":"move_base","slug":"move-base","link":"/tags/move-base/"},{"name":"navfn","slug":"navfn","link":"/tags/navfn/"},{"name":"teb_local_planner","slug":"teb-local-planner","link":"/tags/teb-local-planner/"},{"name":"ArUco","slug":"ArUco","link":"/tags/ArUco/"},{"name":"perception","slug":"perception","link":"/tags/perception/"},{"name":"stereo vision","slug":"stereo-vision","link":"/tags/stereo-vision/"},{"name":"pose estimation","slug":"pose-estimation","link":"/tags/pose-estimation/"},{"name":"OpenCV","slug":"OpenCV","link":"/tags/OpenCV/"},{"name":"calibration","slug":"calibration","link":"/tags/calibration/"},{"name":"ARC 2024","slug":"ARC-2024","link":"/tags/ARC-2024/"},{"name":"Algerian Robot Cup","slug":"Algerian-Robot-Cup","link":"/tags/Algerian-Robot-Cup/"},{"name":"teleoperation","slug":"teleoperation","link":"/tags/teleoperation/"},{"name":"environment","slug":"environment","link":"/tags/environment/"},{"name":"water","slug":"water","link":"/tags/water/"}],"categories":[{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"project","slug":"project","link":"/categories/project/"},{"name":"essay","slug":"essay","link":"/categories/essay/"},{"name":"reading","slug":"reading","link":"/categories/reading/"}],"pages":[{"title":"","text":"Not found404Sorry, the page you requested was not found.","link":"/404.html"},{"title":"Mohamed Reda Touil","text":"Photo in Paris, 2025 Who am II‚Äôm Mohamed Reda Touil, an Electronics Engineer. My research and engineering interests include machine learning, artificial intelligence, computer vision, and medical imaging. I graduated from √âcole Nationale Polytechnique d‚ÄôAlger (ENP) with a Master‚Äôs degree and a Dipl√¥me d‚ÄôIng√©nieur d‚Äô√âtat. I hold myself to very high standards and care deeply about quality. At the same time, I ship pragmatically when perfect solutions aren‚Äôt feasible. Contact Email: mohamed_reda.touil@g.enp.edu.dz Phone: +213 549 446 315 Location: Algeria About this siteI use this space to: Share practical notes on AI/ML/computer vision and medical imaging Document implementations, experiments, and lessons learned Present independent projects and results Record milestones and resources for future me (and you) If you have feedback or questions, feel free to reach out by email.","link":"/index.html"},{"title":"Blog","text":"Redirecting...","link":"/blog/index.html"},{"title":"About","text":"I build AI/ML software for medical imaging and startups.","link":"/about/index.html"},{"title":"Contact","text":"Email me at mohamed_reda.touil@g.enp.edu.dz.","link":"/contact/index.html"},{"title":"archives","text":"","link":"/archives/index-1.html"},{"title":"Archives","text":"","link":"/archives/index.html"},{"title":"categories","text":"","link":"/categories/index-1.html"},{"title":"Categories","text":"","link":"/categories/index.html"},{"title":"faq","text":"","link":"/faq/index-1.html"},{"title":"FAQ","text":"Common questions and answers.","link":"/faq/index.html"},{"title":"essay","text":"","link":"/essay/index-1.html"},{"title":"Essay","text":"Long-form essays.","link":"/essay/index.html"},{"title":"curriculum","text":"","link":"/curriculum/index-1.html"},{"title":"Curriculum Vitae","text":"EDUCATION Mathematics Baccalaureate ‚Äî 2019, NationalAverage: 15.25 ¬∑ Honors: Good Preparatory Classes Student ‚Äî 2019‚Äì2022, ENSTP (COUBA), Algiers Competitive Engineering Exam ‚Äî 2022, NationalRanked 120th out of 1576 Electronics Engineering Student ‚Äî 2022‚Äì2025, √âcole Nationale Polytechnique, Algiers EXPERIENCE PCB Internship ‚Äî Dec 2022, DC/RD SONATRACH, Boumerdes Design and development of printed circuit boards (PCBs). Satellite Communication Internship ‚Äî Mar 2023, Algeria Telecom Satellite, Lkhedaria Explored fundamentals of satellite communication systems. DSP Microcontroller Programming ‚Äî Dec 2023, DC/RD SONATRACH, Boumerdes Programmed the dsPIC30F4011 microcontroller. Instrumentation &amp; PLCs ‚Äî Mar 2024, Algerian Petroleum Institute (SONATRACH) Programming of Siemens PLCs. Research Intern, CDTA (Eurobot) ‚Äî Dec 2024, Algiers Contributing to environmental perception for a mobile robot (Eurobot 2025). Final Year Research Project (PFE) ‚Äî Jun 2024 ‚Äì Jun 2025, ENP (Algiers) &amp; CHU Frantz Fanon (Blida) Automated 3D liver segmentation with deep learning; integrated into a mixed-reality app for pre-operative surgical planning (with the General Surgery Department). SKILLS Programming: Python ¬∑ C/C++/C# ¬∑ Arduino ¬∑ LaTeX Electronics / Robotics: ROS ¬∑ KiCad ¬∑ CUDA ¬∑ Sensor Fusion Machine Learning / CV: PyTorch ¬∑ OpenCV ¬∑ Computer Vision LANGUAGES Arabic ‚Äî Native French ‚Äî TCF SO B2 (414) English","link":"/curriculum/index.html"},{"title":"life","text":"","link":"/life/index-1.html"},{"title":"Life","text":"Personal notes and photos.","link":"/life/index.html"},{"title":"An Article-Style Post","text":"Article-style long post‚Ä¶","link":"/publications/first-article.html"},{"title":"publications","text":"","link":"/publications/index-1.html"},{"title":"Publications","text":"List papers, preprints, posters, and talks.","link":"/publications/index.html"},{"title":"Photography","text":"Galleries or links to albums.","link":"/photography/index.html"},{"title":"photography","text":"","link":"/photography/index-1.html"},{"title":"Readings","text":"Index of reading notes.","link":"/readings/index.html"},{"title":"readings","text":"","link":"/readings/index-1.html"},{"title":"tags","text":"","link":"/tags/index-1.html"},{"title":"Projects","text":"Redirecting...","link":"/projects/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"}]}
{"posts":[{"title":"Model Context Protocol (MCP)","text":"A Protocol for Simplifying LLM Application Development. Why MCP?Before MCP, most LLM apps rebuilt the same glue: Resource collection ‚Äî where to pull context from (files, databases, calendars, screens, etc.) Tool wiring ‚Äî how to call external services (search, code exec, web fetch, RAG, ‚Ä¶) Prompt shape ‚Äî how to consistently describe resources and tools to the model That‚Äôs brittle and expensive. MCP standardizes the surface so apps can: discover what data and tools exist, 2) describe them to the model, and 3) route tool calls‚Äîwithout app-specific adapters. Core concepts Resources ‚Äî things you can read to build context (files, DB tables, dashboards, device state‚Ä¶).Each resource has an ID, human description, optional schema/metadata, and a way to fetch it. Tools ‚Äî actions the model can call (search, translate, execute, query DB, open URL‚Ä¶).A tool has a name, description, arguments schema, and effect flags (e.g., ‚Äúreads external data‚Äù, ‚Äúmutates state‚Äù). Prompts ‚Äî reusable prompt snippets MCP servers can publish (e.g., ‚ÄúSQL safety checklist‚Äù) for high-quality, repeatable guidance. Mental model123LLM app ‚îÄ‚ñ∫ MCP client ‚îÄ‚ñ∫ (one or more) MCP servers ‚îú‚îÄ Resources (discover/fetch) ‚îî‚îÄ Tools (list/execute) Your app speaks only MCP. Servers implement whatever backends they want. Resource examplesMCP doesn‚Äôt force one URI scheme; servers should expose stable, URI-like IDs: 123file:///home/reda/notes/mcp_todos.mdpostgres://prod/orders?table=orders_2025screen://localhost/display1 Minimal resource descriptor (illustrative): 1234567{ &quot;id&quot;: &quot;file:///home/reda/notes/mcp_todos.md&quot;, &quot;title&quot;: &quot;MCP TODOs&quot;, &quot;description&quot;: &quot;Personal notes about tasks and follow-ups.&quot;, &quot;mime&quot;: &quot;text/markdown&quot;, &quot;size&quot;: 4821} Fetching returns a payload (text or bytes) plus metadata: 123456{ &quot;id&quot;: &quot;file:///home/reda/notes/mcp_todos.md&quot;, &quot;content&quot;: &quot;# TODO\\n- write MCP post\\n- push to blog\\n&quot;, &quot;etag&quot;: &quot;b2f9c3&quot;, &quot;updated_at&quot;: &quot;2025-09-14T10:21:00Z&quot;} Tool examplesA tool is described once; the model can then call it through the MCP client. 123456789101112{ &quot;name&quot;: &quot;web_search&quot;, &quot;description&quot;: &quot;Search the web and return top snippets.&quot;, &quot;arguments&quot;: { &quot;query&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Search query&quot; }, &quot;top_k&quot;: { &quot;type&quot;: &quot;integer&quot;, &quot;default&quot;: 5, &quot;minimum&quot;: 1, &quot;maximum&quot;: 20 } }, &quot;effects&quot;: { &quot;reads_external_data&quot;: true, &quot;mutates_state&quot;: false }} Another: safe SQL query tool ‚Äî 12345678910{ &quot;name&quot;: &quot;query_orders&quot;, &quot;description&quot;: &quot;Run a parameterized query on the Orders view.&quot;, &quot;arguments&quot;: { &quot;from&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;format&quot;: &quot;date&quot; }, &quot;to&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;format&quot;: &quot;date&quot; }, &quot;status&quot;: { &quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;open&quot;, &quot;shipped&quot;, &quot;canceled&quot;] } }, &quot;effects&quot;: { &quot;reads_external_data&quot;: true }} Typical client flow Discover ‚Äî list resources/tools from each MCP server. Select ‚Äî for a user query, choose relevant resources (by rules or retrieval) and a small tool set. Describe ‚Äî build a compact context card for the model: names + short descriptions of selected resources/tools any safety constraints / rate limits Call the model ‚Äî provide the user message + context card. Route tool calls ‚Äî execute via the MCP client; capture results (including errors) and stream them back to the model. Answer ‚Äî return final text; optionally attach citations (resource IDs) and traces. Prompt snippetsServers can publish helpful prompts the client can insert: 12345{ &quot;name&quot;: &quot;sql_safety_prompt&quot;, &quot;description&quot;: &quot;Safety checklist for SQL generation&quot;, &quot;arguments&quot;: []} Example usage in a system message: When using query_orders, never select *. Only named columns and limit to 200 rows.Use prepared parameters {from}, {to}, {status}. Minimal ‚ÄúHello MCP‚Äù (pseudo-code)12345678910111213141516171819202122user = &quot;Show shipped orders this week and summarize revenue.&quot;servers = mcp.discover() # list MCP serversresources = mcp.search_resources(&quot;orders&quot;) # pick relevant resourcestools = [&quot;query_orders&quot;] # expose a small, safe toolsetcontext_card = { &quot;resources&quot;: [r.brief() for r in resources], &quot;tools&quot;: [mcp.describe_tool(t) for t in tools], &quot;prompts&quot;: [mcp.get_prompt(&quot;sql_safety_prompt&quot;)]}reply = llm.chat( system = build_system(context_card), user = user, tools = tools)for call in reply.tool_calls: res = mcp.run(call.name, call.args) reply = llm.continue_with_tool_result(call.id, res)print(reply.final_text) Design tips Keep the context small. Prefer brief descriptors + on-demand fetching. Constrain tools. Expose only what‚Äôs needed per turn; validate arguments against the schema. Trace everything. Log resource IDs, tool calls, and inputs/outputs for reproducibility. Fail loudly. Propagate errors to the model with clear messages; retry selectively. Cache. De-duplicate resource fetches by etag or hash. ConclusionsMCP reduces bespoke glue in LLM apps. You standardize how you discover data/tools, describe them to the model, and route tool calls. The result is simpler apps, safer execution paths, and easier portability across backends. Next steps (what I‚Äôll build) A small MCP server exposing: orders Postgres view as resources query_orders and web_search tools A client that prints a full trace (resource IDs, tool args, response times) Blog follow-ups: security boundaries, rate-limits, and evals for tool-use References Official docs: https://modelcontextprotocol.io/docs/getting-started/intro","link":"/2025/09/14/Model-Context-Protocol-MCP/"},{"title":"Stereo ArUco Robot Posestimation ‚Äî Eurobot 2025 Field Computer","text":"A field-side tracking system that estimates each robot‚Äôs (x, y, yaw) in real time using a stereo pair of USB cameras and ArUco markers. The map is anchored with four fixed markers; each robot carries one marker on the roof. This project is designed to act as the central compute (referee/telemetry helper) for our Eurobot 2025 team. Goals Robust global pose for one or more robots: (x, y, Œ∏) Simple, low-cost hardware (two USB cameras, off-the-shelf lenses) Quick setup at the venue and fast re-calibration when needed Hardware Stereo camera rig: 2√ó USB cameras on a rigid bar with a fixed baseline Mounting: Overlooking the game area (a small downward tilt is fine) Markers: 4√ó field markers at known field coordinates (map anchors) 1√ó marker per robot (top-mounted, high contrast) Pipeline Stereo calibration Calibrate each camera (intrinsics &amp; distortion) and the extrinsics between them. Use a chessboard (or ChArUco) and store K1, D1, K2, D2, R, T. Reference I used: this YouTube serie (clear and practical):https://www.youtube.com/playlist?list=PL2zRqk16wsdoCCLpou-dGo7QQNks1Ppzo ArUco detection (both images) Detect field anchors and robot markers with cv2.aruco.detectMarkers. Estimate per-marker pose in each camera with cv2.aruco.estimatePoseSingleMarkers. Map frame With the four known field marker IDs and their world poses, compute or refresh the camera-to-world transform (PnP / solvePnP; optionally average from multiple anchors). If desired, fuse left/right views via stereo triangulation for extra robustness. Robot pose Transform the robot marker pose into the world (field) frame ‚Üí (x, y, yaw). Yaw is extracted from the rotation vector (rvec) ‚Üí rotation matrix ‚Üí Z-yaw. Results Stable tracking with centimeter-level repeatability on a 2√ó3 m field (after calibration) Yaw is stable within a few degrees; larger markers and careful placement improve it Supports multiple robots simultaneously (distinct marker IDs) Images from testing Next steps Use a Jetson Nano instead of personal laptops as the field computer Leverage CUDA to parallelize heavy steps and reduce latency Optional: share robot/world poses over Wi-Fi/ROS so our robots know their ownposition and the opponents‚Äô positions in real time","link":"/2025/01/15/aruco-stereo-tracking/"},{"title":"SurgXR ‚Äî 3D Organ Segmentation &amp; XR Surgical Planning","text":"SurgXR transforms CT scans into interactive 3D models for surgical planning.Built to enables faster, safer planning for complex liver and organ surgeries. Clinical ProblemLiver resections demand a detailed understanding of tumor‚Äìvessel relationships.Conventional 2D CT forces mental 3D reconstruction, increasing planning time and margin uncertainty. Our SolutionSurgXR provides: Automatic 3D reconstruction of liver, tumors, and vessels from CT DICOM. Interactive desktop viewer for measurements and annotations. VR planning sessions using Meta Quest 3, enabling collaborative review of surgical approaches and margins. Key differentiators: Hospital data compliance (no cloud transfer). Subscription in DZD, low total cost of ownership. Case registry for continuous model improvement. System Overview Component Description Input CT scan (DICOM) Segmentation AI model for organ and tumor segmentation (PyTorch) 3D Reconstruction Mesh generation and smoothing Viewer Unity-based desktop app XR Mode Multi-user planning via Meta Quest 3 Tech Stack Deep Learning: PyTorch / MONAI for segmentation 3D Reconstruction: Marching Cubes + MeshLab post-processing XR Development: Unity + Meta XR SDK Deployment: Windows + Linux viewer builds; optional hospital server mode Hardware Setup Intel i9 + RTX 3090 workstation Meta Quest 3 headsets (3 units) Workflow Upload anonymized CT (DICOM). Automatic segmentation ‚Üí liver, tumors, vessels. Review and refine if needed. Explore in desktop viewer or shared VR session. Impact Before With SurgXR 2D slices, mental 3D Ready 3D organ, tumors, vessels 60‚Äì90 min planning 20‚Äì30 min planning Single-user planning Multi-user VR session Uncertain margins Visualized, measurable margins Customer segments: hospitals, clinics, universities. Value proposition: faster planning, safer surgery, collaborative XR. Channels: demos, conferences, local partners. WebsiteWebsite: https://surgxr.netlify.app/ AcknowledgementsThanks to: ENP Incubator team and mentors. Partner hospitals and clinicians who contributed pilot data. The XR and AI developer community.","link":"/2025/09/14/demo-project/"},{"title":"Eurobot 2025 ‚Äî Autonomous Mobile Robot (ROS)","text":"A fully ROS-based autonomous robot for Eurobot 2025.We built a robust navigation stack with sensor fusion (wheel odometry + IMU), global/local planning using move_base, and a custom perception package that aligns the robot precisely with the bo√Ætes de conservation (storage boxes) that carry ArUco tags for fast, reliable docking. System overview OS / Framework: Ubuntu + ROS (Noetic/ROS1) Compute: Jetson Nano Sensors: Encoders (odometry), steppers, IMU, RGB camera for ArUco Actuation: 2-wheel differential drive + gripper/mechanism Localization (sensor fusion)We fuse wheel odometry and IMU with robot_localization (EKF).This yields a stable /odometry/filtered and TF from odom ‚Üí base_link. Navigation (move_base) Global planner: navfn Local planner: TEB (teb_local_planner) for smooth, time-elastic trajectories. Perception (ArUco alignment)A lightweight node detects the ArUco tag on the bo√Æte, estimates T(base_link ‚Üí tag) with OpenCV, publishes a target pose in map, then sends a move_base goal.For final centimeters, a short visual-servo velocity loop refines the pose. Big Thanks üíôSincere thanks to: Talebot team ‚Äî for the long nights, tests, and constant support. Ecole Nationale Polytechnique and all sponsors how aide this project. Mentors, volunteers, and friends who gave feedback, tools, and encouragement. Everyone following the project ‚Äî your interest keeps us building!","link":"/2025/05/20/eurobot-2025-autonomy/"},{"title":"Essay: Why Imperfect Action Beats Perfect Inaction","text":"Your essay text‚Ä¶","link":"/2025/09/12/first-essay/"},{"title":"Reading Notes: A Paper I Like","text":"Your reading notes‚Ä¶","link":"/2025/09/12/first-reading/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2025/09/13/hello-world/"},{"title":"Ocean Cleaning Robot ‚Äî Algerian Robot Cup 2024","text":"Tele-operated robot built for the Algerian Robot Cup (ARC) 2024 to collect floating waste from water surfaces and deliver it to a collection zone. Reliable, easy to service between runs, and tuned for smooth operator control. Competition context Event: Algerian Robot Cup (ARC) 2024 Challenge: Remove floating debris from a pool and deposit it in a target area within the time limit Location: Algiers, Algeria Role: Mechanical &amp; electronics design, integration, and driver Mechanical design Amphibious chassis that can traverse gravel and sand near the water‚Äôs edge and operate effectively on the water surface Wide funnel/scoop guiding debris into a removable basket for fast turn-around Serviceability: quick-release modules and accessible wiring to minimize pit time Electronics &amp; control Land drive: 4√ó DC motors for traction on gravel and sand Water propulsion: 2√ó brushless motors + ESCs for maneuvering in water Actuation: high-torque servo for scoop angle; stepper motor to eject (throw) stacked dishes into the trash basket Tele-op link: nRF24L01 2.4 GHz; tuned with input expo/rate limiting for smooth control Results Consistent pickups across mixed debris sizes Only team to pick multiple dishes, stack them on board, and then throw them all into the trash at once Drawback: the robot was heavy, so top speed was lower Lessons learned Reduce mass (lighter deck, optimized battery size, selective material swaps) to increase speed and maneuverability Improve sealing and splash protection around moving shafts and connectors Add voltage/current telemetry to prevent late-match brownouts Media Photo: Algerian Robot Cup (ARC) 2024.","link":"/2024/07/20/ocean-cleaning-robot/"},{"title":"Line Follower &#x2F; Maze Solver ‚Äî Polymaze 2023","text":"Autonomous line-following robot that detects intersections, maps the maze, then re-runs the shortest path. Built for Polymaze 2023. OverviewThe robot follows a black line using PID on an IR sensor array, detects nodes (R/L/T intersections, turns, and dead ends), saves the path to EEPROM, prunes wrong turns/backtracks, then computes the shortest path and performs an optimized second run. Event: Polymaze 2023 Goal: Fastest complete maze Role: Design, control &amp; software Hardware Microcontroller: Arduino Uno Sensors: 5-element IR array Drive: 2√ó DC motors + L298N driver Power: 3S LiPo Chassis: Laser-cut Control &amp; SensingLine tracking (PID): proportional‚Äìintegral‚Äìderivative control on the line error from the IR array.Node detection: state machine for junction types (R/L/T/cross), debounced by distance/time.Mapping: record moves as symbols (e.g., L, R, S, B for backtrack) in EEPROM.Path optimization: compress the sequence by removing backtrack pairs and simplifying to the minimal route for the second run.","link":"/2023/07/20/polymaze-2023/"},{"title":"Welcome to Reda&#39;s Log Book","text":"This site is built with Hexo and the Icarus theme to share my research, projects, and reading notes.","link":"/2025/09/12/welcome/"},{"title":"Sniffing Network Traffic","text":"Read this first: Monitoring or intercepting traffic that isn‚Äôt yours (or without clear written consent) is illegal and unethical. This post is a defensive, educational overview. Try things only on your own devices or in a closed lab you control. TL;DR You can learn a lot by capturing your own device‚Äôs traffic with Wireshark. Sniffing other people‚Äôs traffic on modern switched Wi‚ÄëFi/LANs is not trivial and typically requires a man‚Äëin‚Äëthe‚Äëmiddle (MITM) position (e.g., ARP spoofing) ‚Äî that‚Äôs an attack. I won‚Äôt publish step‚Äëby‚Äëstep exploit commands here. The most useful takeaway: understand what metadata is visible, why encryption (HTTPS/TLS) matters, and how to defend against MITM. The setup (safe, legal)Create a tiny lab: One laptop/desktop (your machine) Optional: a second device you also own (phone/tablet) Both connected to your home router or a separate test AP Install tools: Wireshark ‚Äì packet capture &amp; analysis 1sudo apt install wireshark Nmap ‚Äì inventory devices you own on your test subnet 1sudo apt install nmap Eettercap 1sudo apt install ettercap-text-only Why sniffing is hard on real networksOld hubs echoed every frame to every port ‚Äî trivially sniffable. Today‚Äôs switches forward frames only to the destination MAC, so your NIC generally sees only your own traffic, and broadcast/multicast (e.g., ARP, mDNS, DHCP). To see someone else‚Äôs traffic, an attacker would need to sit in the middle (MITM). A common technique is ARP spoofing/poisoning: trick hosts into sending frames to the attacker by lying about the MAC address for a given IP. This is explicitly malicious and out of scope here. Capture your own traffic with Wireshark Open Wireshark ‚Üí select your active interface (e.g., wlo1 for Wi‚ÄëFi). Start capture and browse a few websites, open an app, etc. Stop capture and explore. Useful display filters http ‚Äì legacy HTTP (rare nowadays) tls ‚Äì TLS handshake and encrypted application data dns ‚Äì classic DNS queries (if not using DoH/DoT) tcp.port == 443 ‚Äì most HTTPS traffic ip.addr == &lt;your_phone_ip&gt; ‚Äì focus on a specific device in your lab Tip: Right‚Äëclick a packet ‚Üí Follow ‚Üí TCP/UDP stream. If the app uses TLS, you‚Äôll see encrypted bytes. You can still learn who talks to whom (SNI/Server IP, cert info) without seeing the content. (High‚Äëlevel) How attackers try to see others‚Äô trafficI won‚Äôt include exploitation commands, but conceptually: Discovery: find devices on the subnet. Positioning: become the gateway for a victim and/or the router (e.g., ARP spoofing) to proxy traffic. Inspection/Modification: sniff or alter frames while forwarding. This is illegal without explicit permission. Instead, focus on the defenses below. Quick, safe commands referenceThese are fine to run on your own lab only: 12345# List interfaces and IPsip a# Inventory your *own* test subnetnmap -sn 192.168.1.0/24 # replace with your lab range About MITM/Ettercap (why details are redacted)Placing yourself in the middle of other devices‚Äô traffic requires ARP spoofing or similar techniques. Sharing the exact commands would enable harm, so I‚Äôm keeping this section high‚Äëlevel and defensive. If you‚Äôre doing an authorized lab or pentest, consult official docs and get written permission first. Below is an example CLI output from a controlled lab run for educational discussion: Inspect traffic with WiresharkStart Wireshark and select your interface: 1sudo wireshark Select the wlo1 interface to start capturing: You‚Äôll see traffic like this: Apply a display filter to focus on a specific device (example): Follow a UDP stream (conversation): You‚Äôll notice the payload is unreadable ‚Äî it‚Äôs encrypted (good!).Red = client; Blue = server. Note: Not all traffic is encrypted. Legacy http:// sites and some IoT protocols may still be in clear text ‚Äî treat that as sensitive and remediate. References Wireshark User‚Äôs Guide ‚Äî https://www.wireshark.org/docs/wsug_html_chunked/ NetworkChuck: https://www.youtube.com/watch?v=-rSqbgI7oZM&amp;t=315s","link":"/2025/09/14/sniffing-network-traffic/"}],"tags":[{"name":"MCP","slug":"MCP","link":"/tags/MCP/"},{"name":"Model Context Protocol","slug":"Model-Context-Protocol","link":"/tags/Model-Context-Protocol/"},{"name":"LLM","slug":"LLM","link":"/tags/LLM/"},{"name":"tools","slug":"tools","link":"/tags/tools/"},{"name":"context","slug":"context","link":"/tags/context/"},{"name":"Eurobot 2025","slug":"Eurobot-2025","link":"/tags/Eurobot-2025/"},{"name":"robotics","slug":"robotics","link":"/tags/robotics/"},{"name":"ArUco","slug":"ArUco","link":"/tags/ArUco/"},{"name":"stereo vision","slug":"stereo-vision","link":"/tags/stereo-vision/"},{"name":"pose estimation","slug":"pose-estimation","link":"/tags/pose-estimation/"},{"name":"OpenCV","slug":"OpenCV","link":"/tags/OpenCV/"},{"name":"calibration","slug":"calibration","link":"/tags/calibration/"},{"name":"Medical Imaging","slug":"Medical-Imaging","link":"/tags/Medical-Imaging/"},{"name":"XR","slug":"XR","link":"/tags/XR/"},{"name":"3D Segmentation","slug":"3D-Segmentation","link":"/tags/3D-Segmentation/"},{"name":"Unity","slug":"Unity","link":"/tags/Unity/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Startup","slug":"Startup","link":"/tags/Startup/"},{"name":"ROS","slug":"ROS","link":"/tags/ROS/"},{"name":"robot_localization","slug":"robot-localization","link":"/tags/robot-localization/"},{"name":"EKF","slug":"EKF","link":"/tags/EKF/"},{"name":"IMU","slug":"IMU","link":"/tags/IMU/"},{"name":"odometry","slug":"odometry","link":"/tags/odometry/"},{"name":"move_base","slug":"move-base","link":"/tags/move-base/"},{"name":"navfn","slug":"navfn","link":"/tags/navfn/"},{"name":"teb_local_planner","slug":"teb-local-planner","link":"/tags/teb-local-planner/"},{"name":"perception","slug":"perception","link":"/tags/perception/"},{"name":"thoughts","slug":"thoughts","link":"/tags/thoughts/"},{"name":"notes","slug":"notes","link":"/tags/notes/"},{"name":"ARC 2024","slug":"ARC-2024","link":"/tags/ARC-2024/"},{"name":"Algerian Robot Cup","slug":"Algerian-Robot-Cup","link":"/tags/Algerian-Robot-Cup/"},{"name":"teleoperation","slug":"teleoperation","link":"/tags/teleoperation/"},{"name":"environment","slug":"environment","link":"/tags/environment/"},{"name":"water","slug":"water","link":"/tags/water/"},{"name":"line follower","slug":"line-follower","link":"/tags/line-follower/"},{"name":"maze","slug":"maze","link":"/tags/maze/"},{"name":"embedded","slug":"embedded","link":"/tags/embedded/"},{"name":"polymaze","slug":"polymaze","link":"/tags/polymaze/"},{"name":"welcome","slug":"welcome","link":"/tags/welcome/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"networking","slug":"networking","link":"/tags/networking/"},{"name":"Wireshark","slug":"Wireshark","link":"/tags/Wireshark/"},{"name":"security","slug":"security","link":"/tags/security/"},{"name":"ARP","slug":"ARP","link":"/tags/ARP/"},{"name":"encryption","slug":"encryption","link":"/tags/encryption/"},{"name":"privacy","slug":"privacy","link":"/tags/privacy/"}],"categories":[{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"project","slug":"project","link":"/categories/project/"},{"name":"essay","slug":"essay","link":"/categories/essay/"},{"name":"reading","slug":"reading","link":"/categories/reading/"}],"pages":[{"title":"Mohamed Reda Touil","text":"Photo in Paris, 2025 Who am II‚Äôm Mohamed Reda Touil, an Electronics Engineer. My research and engineering interests include machine learning, artificial intelligence, computer vision, and medical imaging. I graduated from √âcole Nationale Polytechnique d‚ÄôAlger (ENP) with a Master‚Äôs degree and a Dipl√¥me d‚ÄôIng√©nieur d‚Äô√âtat. I hold myself to very high standards and care deeply about quality. At the same time, I ship pragmatically when perfect solutions aren‚Äôt feasible. Contact Email: mohamed_reda.touil@g.enp.edu.dz Phone: +213 549 446 315 Location: Algeria About this siteI use this space to: Share practical notes on AI/ML/computer vision and medical imaging Document implementations, experiments, and lessons learned Present independent projects and results Record milestones and resources for future me (and you) If you have feedback or questions, feel free to reach out by email.","link":"/index.html"},{"title":"About","text":"I build AI/ML software for medical imaging and startups.","link":"/about/index.html"},{"title":"archives","text":"","link":"/archives/index-1.html"},{"title":"Archives","text":"","link":"/archives/index.html"},{"title":"Blog","text":"Redirecting...","link":"/blog/index.html"},{"title":"","text":"/* Make the profile widget avatar circular */ .widget-profile img, .widget .profile img, .card-widget .avatar img, .profile .avatar img { border-radius: 50% !important; /* circle */ aspect-ratio: 1 / 1; /* force square box */ width: 160px; /* adjust as you like */ height: 160px; /* keeps it perfectly round */ object-fit: cover; /* crop to center nicely */ box-shadow: 0 0 0 4px var(--card-bg, #fff); /* subtle white ring */ } /* Tweak size on small screens */ @media (max-width: 480px) { .widget-profile img, .widget .profile img, .card-widget .avatar img, .profile .avatar img { width: 120px; height: 120px; } }","link":"/css/custom.css"},{"title":"","text":":root{ --nav-bg: rgba(11,23,40,.75); --nav-border: rgba(255,255,255,.06); --brand:#6cc1c4; } .navbar{ position:sticky; top:0; z-index:9999; backdrop-filter:saturate(140%) blur(10px); -webkit-backdrop-filter:saturate(140%) blur(10px); background:var(--nav-bg)!important; border-bottom:1px solid var(--nav-border); } .navbar-brand .navbar-item{ font-weight:800; letter-spacing:.3px; color:#e6f1ff; } .navbar-brand .navbar-item::before{ content:\"\"; display:inline-block; width:18px; height:18px; margin-right:.5rem; border-radius:50%; background:radial-gradient(circle at 30% 30%, var(--brand), transparent 60%), conic-gradient(from 210deg,#2ad7e2,#0bc 25%,#7fffd4 50%,#2ad7e2 75%,#0bc); box-shadow:0 0 16px rgba(108,193,196,.45); } .navbar-menu .navbar-item{ border-radius:12px; margin:0 .25rem; padding:.55rem .9rem; color:#e8f1ff; } .navbar-menu .navbar-item:hover{ background:rgba(255,255,255,.08) } .navbar-menu .navbar-item.is-active{ background:rgba(108,193,196,.18); box-shadow:inset 0 0 0 1px rgba(108,193,196,.35); } .navbar-burger span{ background:#cde9ff !important } @media (min-width:1024px){ .navbar-menu{ justify-content:center; } }","link":"/css/nav.css"},{"title":"categories","text":"","link":"/categories/index-1.html"},{"title":"Categories","text":"","link":"/categories/index.html"},{"title":"curriculum","text":"","link":"/curriculum/index-1.html"},{"title":"Curriculum Vitae","text":"EDUCATION Mathematics Baccalaureate ‚Äî 2019, NationalAverage: 15.25 ¬∑ Honors: Good Preparatory Classes Student ‚Äî 2019‚Äì2022, ENSTP (COUBA), Algiers Competitive Engineering Exam ‚Äî 2022, NationalRanked 120th out of 1576 Electronics Engineering Student ‚Äî 2022‚Äì2025, √âcole Nationale Polytechnique, Algiers EXPERIENCE PCB Internship ‚Äî Dec 2022, DC/RD SONATRACH, BoumerdesDesign and development of printed circuit boards (PCBs). Satellite Communication Internship ‚Äî Mar 2023, Algeria Telecom Satellite, LkhedariaExplored fundamentals of satellite communication systems. DSP Microcontroller Programming ‚Äî Dec 2023, DC/RD SONATRACH, BoumerdesProgrammed the dsPIC30F4011 microcontroller. Instrumentation &amp; PLCs ‚Äî Mar 2024, Algerian Petroleum Institute (SONATRACH)Programming of Siemens PLCs. Research Intern, CDTA (Eurobot) ‚Äî Dec 2024, AlgiersContributing to environmental perception for a mobile robot (Eurobot 2025). Final Year Research Project (PFE) ‚Äî Jun 2024 ‚Äì Jun 2025, ENP (Algiers) &amp; CHU Frantz Fanon (Blida)Automated 3D liver segmentation with deep learning; integrated into a mixed-reality app for pre-operative surgical planning (with the General Surgery Department). SKILLS Programming: Python ¬∑ C/C++/C# ¬∑ Arduino ¬∑ LaTeX Electronics / Robotics: ROS ¬∑ KiCad ¬∑ CUDA ¬∑ Sensor Fusion Machine Learning / CV: PyTorch ¬∑ OpenCV ¬∑ Computer Vision LANGUAGES Arabic ‚Äî Native French ‚Äî TCF SO B2 (414) English","link":"/curriculum/index.html"},{"title":"essay","text":"","link":"/essay/index-1.html"},{"title":"Essay","text":"Long-form essays.","link":"/essay/index.html"},{"title":"Contact","text":"Email me at mohamed_reda.touil@g.enp.edu.dz.","link":"/contact/index.html"},{"title":"faq","text":"","link":"/faq/index-1.html"},{"title":"FAQ","text":"Common questions and answers.","link":"/faq/index.html"},{"title":"Projects","text":"Redirecting...","link":"/projects/index.html"},{"title":"life","text":"","link":"/life/index-1.html"},{"title":"Life","text":"Personal notes and photos.","link":"/life/index.html"},{"title":"An Article-Style Post","text":"Article-style long post‚Ä¶","link":"/publications/first-article.html"},{"title":"publications","text":"","link":"/publications/index-1.html"},{"title":"Publications","text":"List papers, preprints, posters, and talks.","link":"/publications/index.html"},{"title":"photography","text":"","link":"/photography/index-1.html"},{"title":"Photography","text":"Galleries or links to albums.","link":"/photography/index.html"},{"title":"readings","text":"","link":"/readings/index-1.html"},{"title":"Readings","text":"Index of reading notes.","link":"/readings/index.html"},{"title":"tags","text":"","link":"/tags/index-1.html"},{"title":"Tags","text":"","link":"/tags/index.html"},{"title":"","text":"Not found404Sorry, the page you requested was not found.","link":"/404.html"}]}